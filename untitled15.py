# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WIq7jNPuUIdAzTp8uqeCNUZUUL_vGOSl
"""

!pip install torch transformers seaborn tqdm matplotlib numpy seaborn accelerate

!huggingface-cli login

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf" , torch_dtype=torch.float16, device_map="auto")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf" , torch_dtype=torch.float16, device_map="auto")

def split_decoder_layers(model, x, y):
    head_layers = torch.nn.ModuleList(model.model.decoder.layers[:x])
    sub_body_layers = torch.nn.ModuleList(model.model.decoder.layers[x:y])
    tail_layers = torch.nn.ModuleList(model.model.decoder.layers[y:])
    return head_layers, sub_body_layers, tail_layers

def process_head(input_prompt, head_layers, model):
    input_ids = tokenizer(input_prompt, return_tensors='pt').input_ids
    embedding = model.model.embed_tokens(input_ids)
    for layer in head_layers:
        embedding = layer(embedding)
    return embedding

def process_sub_body_locally(embedding, sub_body_layers):
    for layer in sub_body_layers:
        embedding = layer(embedding)
    return embedding

def process_tail(embedding, tail_layers, model):
    for layer in tail_layers:
        embedding = layer(embedding)
    logits = model.lm_head(embedding)
    sampled_output = torch.argmax(logits, dim=-1)
    response = tokenizer.decode(sampled_output[0], skip_special_tokens=True)
    return response

import time

def measure_performance(input_prompt, x, y, model):
    head_layers, sub_body_layers, tail_layers = split_decoder_layers(model, x, y)

    # Measuring head processing time
    start_local_head = time.time()
    embedding = process_head(input_prompt, head_layers, model)
    end_local_head = time.time()

    # Measuring sub-body processing time
    start_sub_body = time.time()
    processed_embedding = process_sub_body_locally(embedding, sub_body_layers)
    end_sub_body = time.time()

    # Measuring tail processing time
    start_local_tail = time.time()
    response = process_tail(processed_embedding, tail_layers, model)
    end_local_tail = time.time()

    local_time = (end_local_head - start_local_head) + (end_local_tail - start_local_tail)
    sub_body_time = end_sub_body - start_sub_body
    total_time = local_time + sub_body_time

    return {
        "local_head_time": end_local_head - start_local_head,
        "sub_body_time": sub_body_time,
        "local_tail_time": end_local_tail - start_local_tail,
        "total_time": total_time,
        "response": response
    }

import sys

def calculate_bandwidth(data):
    return sys.getsizeof(data) / (1024 * 1024)  # in MB

layer_ratios = [(0, 32), (2, 30), (4, 28), (8, 24), (16, 16), (24, 8), (32, 0)]
input_prompt = "Tell me about Samsung Research Institute Bangalore"

performance_metrics = []

for x, y in layer_ratios:
    metrics = measure_performance(input_prompt, x, y, model)
    metrics["layer_ratio"] = f"{x}:{32-x}"
    performance_metrics.append(metrics)

for metric in performance_metrics:
    print(metric)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
import sys
from tqdm import tqdm

# Loading tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf" , torch_dtype=torch.float16, device_map="auto")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf" , torch_dtype=torch.float16, device_map="auto")

def split_decoder_layers(model, x, y):
    # Extract the transformer layers for the splitting of the decode layers [0:x] , [x:y] , [y:32]
    transformer_layers = model.model.layers

    
    head_layers = torch.nn.ModuleList(transformer_layers[:x]) #first layer
    sub_body_layers = torch.nn.ModuleList(transformer_layers[x:y]) #second layer
    tail_layers = torch.nn.ModuleList(transformer_layers[y:]) #last layer

    return head_layers, sub_body_layers, tail_layers

def calculate_data_transferred(tensor):
    # Calculate data size of embeddings to calculate data transferred in MB
    data_size_MB = tensor.element_size() * tensor.nelement() / (1024 * 1024)
    return data_size_MB

def process_head(input_prompt, head_layers, model):
    input_ids = tokenizer(input_prompt, return_tensors='pt').input_ids
    position_ids = torch.arange(0, input_ids.size(1), dtype=torch.long).unsqueeze(0)
    embedding = model.model.embed_tokens(input_ids)
    for layer in head_layers:
        embedding = layer(embedding, position_ids=position_ids)[0]
    return embedding, position_ids

def process_sub_body_locally(embedding, sub_body_layers, position_ids):
    for layer in sub_body_layers:
        embedding = layer(embedding, position_ids=position_ids)[0]
    return embedding

def process_tail(embedding, tail_layers, model, position_ids):
    for layer in tail_layers:
        embedding = layer(embedding, position_ids=position_ids)[0]
    logits = model.lm_head(embedding)
    sampled_output = torch.argmax(logits, dim=-1)
    response = tokenizer.decode(sampled_output[0], skip_special_tokens=True)
    return response

def measure_performance(input_prompt, x, y, model):
    head_layers, sub_body_layers, tail_layers = split_decoder_layers(model, x, y)

    # Initialize variables to track data transferred
    uplink_data_MB = 0.0
    downlink_data_MB = 0.0

    # Measuring head processing time and data transferred
    start_local_head = time.time()
    embedding, position_ids = process_head(input_prompt, head_layers, model)
    end_local_head = time.time()
    uplink_data_MB += calculate_data_transferred(embedding)

    # Measuring sub-body processing time and data transferred
    start_sub_body = time.time()
    processed_embedding = process_sub_body_locally(embedding, sub_body_layers, position_ids)
    end_sub_body = time.time()
    downlink_data_MB += calculate_data_transferred(processed_embedding)

    # Measuring tail processing time
    start_local_tail = time.time()
    response = process_tail(processed_embedding, tail_layers, model, position_ids)
    end_local_tail = time.time()

    # Calculate processing times
    local_head_time = end_local_head - start_local_head
    sub_body_time = end_sub_body - start_sub_body
    local_tail_time = end_local_tail - start_local_tail
    total_time = local_head_time + sub_body_time + local_tail_time

    # Calculate token generation throughput
    num_tokens = len(tokenizer.encode(response))
    throughput_tokens_per_sec = num_tokens / total_time

    return {
        "layer_ratio": f"{x}:{32-x}",
        "local_head_time": local_head_time,
        "sub_body_time": sub_body_time,
        "local_tail_time": local_tail_time,
        "total_time": total_time,
        "throughput_tokens_per_sec": throughput_tokens_per_sec
    }

# Define layer ratios and input prompt
layer_ratios = [(0, 32), (2, 30), (8, 24), (16, 16), (24,8) , (30,2) , (32, 0)]
input_prompt = "Tell me about Samsung Research Institute Bangalore"

# Measuring performance for each layer ratio
performance_metrics = []
for x, y in tqdm(layer_ratios, desc="Computation layer ratio"):
    metrics = measure_performance(input_prompt, x, y, model)
    performance_metrics.append(metrics)

# Creating the performance chart table in an excel sheet
df = pd.DataFrame(performance_metrics)
df.to_csv("performance_metrics.csv", index=False)

# Ploting Layer Ratio vs Response Time Graph 
df['x_layer'] = df['layer_ratio'].apply(lambda x: int(x.split(':')[0]))
plt.figure(figsize=(10, 6))
plt.plot(df['x_layer'], df['total_time'], marker='o', linestyle='-', color='b')
plt.title('Layer Ratio vs Response Time')
plt.xlabel('Number of Head Layers (x)')
plt.ylabel('Response Time (seconds)')
plt.grid(True)
plt.savefig("layer_ratio_vs_response_time.png")
plt.show()

average_throughput = df['throughput_tokens_per_sec'].mean()
print(f"Average Token Generation Throughput: {average_throughput} tokens/second")

print(df)

